{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e22768",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1d855",
   "metadata": {},
   "source": [
    "공통 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31fb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 가져오기\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#시각화 패키지\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#시각화할 때 한글을 출력하기 위해서\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#한글처리\n",
    "#메켄도시의 경우\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "#윈도우의 경우\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname = \"c://Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63062ca",
   "metadata": {},
   "source": [
    "수치 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9e341",
   "metadata": {},
   "source": [
    "이상치 감지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614381ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1], dtype=int64), array([0], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#z-score를 이용하는 방법\n",
    "#z-score:(데이터-평균)/표준편차가 절대값으로 3이 넘는 경우 이상치\n",
    "#z-score를 이용한 이상치 감지를 위한 함수\n",
    "def outliers_z_score(ys):\n",
    "    #임계값 설정\n",
    "    threshold = 3\n",
    "    \n",
    "    #평균 구하기\n",
    "    mean_y = np.mean(ys)\n",
    "    \n",
    "    #표준편차 구하기\n",
    "    stdev_y = np.std(ys)\n",
    "    \n",
    "    #z_score 구하기\n",
    "    z_scores = [(y - mean_y) / stdev_y for y in ys]\n",
    "    \n",
    "    return np.where(np.abs(z_scores) > threshold)\n",
    "\n",
    "features = np.array([[10, 10, 7, 6, 4, 5, 3, 3], [15000, 10, 7, 6, 4, 2, 2, 2]])\n",
    "print(outliers_z_score(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c802ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1], dtype=int64), array([0], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#z-score를 보장 - MAD(중위 절대 편차)\n",
    "def outliers_modified_z_score(ys):\n",
    "    #임계값 설정\n",
    "    threshold = 3.5\n",
    "    \n",
    "    #중앙값 구하기\n",
    "    median_y = np.median(ys)\n",
    "    \n",
    "    #편차값 구하기\n",
    "    median_absolute_deviation_y = np.median([np.abs(y-median_y) for y in ys])\n",
    "    \n",
    "    #보정한 z_score 구하기\n",
    "    median_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y for y in ys]\n",
    "   \n",
    "    #이상치를 검출한 후 인덱스를 리턴\n",
    "    return np.where(np.abs(median_z_scores) > threshold)\n",
    "\n",
    "features = np.array([[10, 10, 7, 6, 4, 5, 3, 3], [15000, 10, 7, 6, 4, 2, 2, 2]])\n",
    "print(outliers_modified_z_score(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89758af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하한: -4.125\n",
      "상한: 14.875\n",
      "(array([1], dtype=int64), array([0], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#IQR을 이용하는 방법 - (3사분위수 - 1사분위수)\n",
    "#1사분위 수에서 IQR*1.5 한 값을 뺀 것보다 적거나\n",
    "#3사분위 수에서 IQR*1.5 한 값을 더한 것보다 큰 경우\n",
    "def outliers_iqr(ys):\n",
    "    #1사분위수와 3사분위수 구하기\n",
    "    q1, q3 = np.percentile(ys, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    #경계값 계산\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    print(\"하한:\", lower_bound)\n",
    "    print(\"상한:\", upper_bound)\n",
    "    \n",
    "    #이상치를 검출한 후 인덱스를 리턴\n",
    "    return np.where((ys < lower_bound) | (ys > upper_bound))\n",
    "\n",
    "features = np.array([[10, 10, 7, 6, 4, 5, 3, 3], [15000, 10, 7, 6, 4, 2, 2, 2]])\n",
    "print(outliers_iqr(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db709b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.743351    8.78014917]\n",
      " [-3.4172217   7.60198243]\n",
      " [-3.52202874  9.32853346]\n",
      " [-2.26723535  7.10100588]\n",
      " [-2.97261532  8.54855637]\n",
      " [-1.04354885  8.78850983]\n",
      " [-1.86150908 10.53731598]\n",
      " [-2.97867201  9.55684617]\n",
      " [-4.23411546  8.4519986 ]\n",
      " [-0.92998481  9.78172086]]\n",
      "[[-2.74335100e+00  8.78014917e+00]\n",
      " [ 2.00000000e+03  1.50000000e+03]\n",
      " [-3.52202874e+00  9.32853346e+00]\n",
      " [-2.26723535e+00  7.10100588e+00]\n",
      " [-2.97261532e+00  8.54855637e+00]\n",
      " [-1.04354885e+00  8.78850983e+00]\n",
      " [-1.86150908e+00  1.05373160e+01]\n",
      " [-2.97867201e+00  9.55684617e+00]\n",
      " [-4.23411546e+00  8.45199860e+00]\n",
      " [-9.29984808e-01  9.78172086e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:647: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
      "  warnings.warn(\"The covariance matrix associated to your dataset \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일정 비율을 이상치로 설정\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "#10퍼센트를 이상치로 간주하는 객체 생성\n",
    "outlier_detector = EllipticEnvelope(contamination = 0.1)\n",
    "\n",
    "#샘플 데이터 생성\n",
    "from sklearn.datasets import make_blobs\n",
    "features, _ = make_blobs(n_samples = 10,\n",
    "                        n_features = 2,\n",
    "                         centers = 1,\n",
    "                         random_state = 42)\n",
    "print(features)\n",
    "\n",
    "#1번 행의 데이터를 이상치로 변경\n",
    "features[1, 0] = 2000\n",
    "features[1, 1] = 1500\n",
    "print(features)\n",
    "\n",
    "features = np.array([[10, 10, 7, 6, 4, 5, 3, 3], [15000, 10, 7, 6, 4, 2, 2, 2]])\n",
    "#훈련\n",
    "outlier_detector.fit(features)\n",
    "#예측\n",
    "outlier_detector.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfbbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91af610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Price  Bathrooms  Square_Feet\n",
      "0   534433        2.0         1500\n",
      "1   392333        3.5         2500\n",
      "2   293222        2.0         1500\n",
      "3  4322032      116.0        48000\n",
      "\n",
      "     Price  Bathrooms  Square_Feet  Outlier\n",
      "0   534433        2.0         1500        0\n",
      "1   392333        3.5         2500        0\n",
      "2   293222        2.0         1500        0\n",
      "3  4322032      116.0        48000        1\n",
      "\n",
      "     Price  Bathrooms  Square_Feet  Outlier  Log_Of_Square_Feet\n",
      "0   534433        2.0         1500        0            7.313220\n",
      "1   392333        3.5         2500        0            7.824046\n",
      "2   293222        2.0         1500        0            7.313220\n",
      "3  4322032      116.0        48000        1           10.778956\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18104/3639311782.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#이상치에 덜 민감한 스케일링\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRobustScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhouses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bathrooms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhouses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bathrooms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "houses = pd.DataFrame()\n",
    "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
    "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
    "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
    "print(houses)\n",
    "print()\n",
    "\n",
    "#새로운 특성 추가 - bathrooms가 20이 넘는 경우 새로운 특성으로 변환\n",
    "# 불리언 조건을 기반으로 특성을 만듭니다.\n",
    "houses[\"Outlier\"] = np.where(houses[\"Bathrooms\"] < 20, 0, 1)\n",
    "# 데이터를 확인합니다.\n",
    "print(houses)\n",
    "print()\n",
    "\n",
    "#특성 변환 - 로그 특성\n",
    "houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]]\n",
    "# 데이터를 확인합니다.\n",
    "print(houses)\n",
    "print()\n",
    "\n",
    "#스케일링\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#이상치에 덜 민감한 스케일링\n",
    "scaler = sklearn.preprocessing.RobustScaler()\n",
    "scaler.fit(houses[['bathrooms']])\n",
    "print(scaler.transform(houses[['bathrooms']]))\n",
    "\n",
    "#이상치에 민감한 스케일링\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(houses[['bathrooms']])\n",
    "print(scaler.transform(houses[['bathrooms']]))\n",
    "\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "scaler.fit(houses[['bathrooms']])\n",
    "print(scaler.transform(houses[['bathrooms']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333eea98",
   "metadata": {},
   "source": [
    "결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab8bf0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n",
      "C    59\n",
      "B    47\n",
      "D    33\n",
      "E    32\n",
      "A    15\n",
      "F    13\n",
      "G     4\n",
      "Name: deck, dtype: int64\n",
      "NaN    688\n",
      "C       59\n",
      "B       47\n",
      "D       33\n",
      "E       32\n",
      "A       15\n",
      "F       13\n",
      "G        4\n",
      "Name: deck, dtype: int64\n",
      "688\n"
     ]
    }
   ],
   "source": [
    "#seaborn의 titanic 데이터 가져오기\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "#RangeIndex의 개수와 열의 데이터 개수가 맞지 않으면 결측치가 존재\n",
    "titanic.info()\n",
    "\n",
    "#각 데이터 개수를 리턴 - 옵션이 없으면 None은 제외\n",
    "print(titanic['deck'].value_counts())\n",
    "\n",
    "#None을 제외하지 않고 리턴\n",
    "print(titanic['deck'].value_counts(dropna = False))\n",
    "\n",
    "#isnull 이용\n",
    "print(titanic['deck'].isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af6522",
   "metadata": {},
   "source": [
    "결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69fcdd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1 11.1]\n",
      " [ 2.2 22.2]\n",
      " [ 3.3 33.3]\n",
      " [ 4.4 44.4]\n",
      " [ nan 55.5]]\n",
      "\n",
      "[[ 1.1 11.1]\n",
      " [ 2.2 22.2]\n",
      " [ 3.3 33.3]\n",
      " [ 4.4 44.4]]\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'embark_town', 'alive',\n",
      "       'alone'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  embark_town  889 non-null    object  \n",
      " 12  alive        891 non-null    object  \n",
      " 13  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(1), float64(2), int64(4), object(5)\n",
      "memory usage: 79.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 714 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     714 non-null    int64   \n",
      " 1   pclass       714 non-null    int64   \n",
      " 2   sex          714 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        714 non-null    int64   \n",
      " 5   parch        714 non-null    int64   \n",
      " 6   fare         714 non-null    float64 \n",
      " 7   embarked     712 non-null    object  \n",
      " 8   class        714 non-null    category\n",
      " 9   who          714 non-null    object  \n",
      " 10  adult_male   714 non-null    bool    \n",
      " 11  embark_town  712 non-null    object  \n",
      " 12  alive        714 non-null    object  \n",
      " 13  alone        714 non-null    bool    \n",
      "dtypes: bool(2), category(1), float64(2), int64(4), object(5)\n",
      "memory usage: 69.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#결측치 제거\n",
    "import numpy as np\n",
    "features = np.array([\n",
    "    [1.1, 11.1],[2.2, 22.2],[3.3, 33.3],[4.4, 44.4],[np.nan, 55.5]])\n",
    "print(features)\n",
    "print()\n",
    "#(~ 연산자를 사용하여) None 제거\n",
    "#isnan이 True인 데이터를 제외하고 가져오기\n",
    "print(features[~np.isnan(features).any(axis=1)])\n",
    "print()\n",
    "\n",
    "titanic.info()\n",
    "#컬럼별 null 데이터의 개수 찾기\n",
    "print(titanic.isnull().sum(axis=0))\n",
    "print()\n",
    "#NaN 값이 500개 이상인 컬럼 제거\n",
    "titanic_thresh = titanic.dropna(axis=1, thresh=500)\n",
    "print(titanic_thresh.columns)\n",
    "titanic_thresh.info()\n",
    "\n",
    "#age가 None인 데이터 삭제\n",
    "titanic_age=titanic_thresh.dropna(subset=['age'], how='any')\n",
    "titanic_age.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2d34d",
   "metadata": {},
   "source": [
    "결측치 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33076d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829     Queenstown\n",
      "830      Cherbourg\n",
      "831    Southampton\n",
      "832      Cherbourg\n",
      "833    Southampton\n",
      "834    Southampton\n",
      "Name: embark_town, dtype: object\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829     Queenstown\n",
      "830      Cherbourg\n",
      "831    Southampton\n",
      "832      Cherbourg\n",
      "833    Southampton\n",
      "834    Southampton\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#829번째 데이터에 NaN이 있음\n",
    "print(titanic['embark_town'][825:835])\n",
    "print()\n",
    "\n",
    "#이전 값으로 \n",
    "#ffill대신 bfill 가능\n",
    "#method를 제거하고 다른 값으로 작성해도 됨\n",
    "titanic_fill_ffill = titanic['embark_town'].fillna(method='ffill', inplace=True)\n",
    "print(titanic['embark_town'][825:835])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48196494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southampton    644\n",
      "Cherbourg      168\n",
      "Queenstown      77\n",
      "Name: embark_town, dtype: int64\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829    Southampton\n",
      "830      Cherbourg\n",
      "831    Southampton\n",
      "832      Cherbourg\n",
      "833    Southampton\n",
      "834    Southampton\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#가장 자주 등장하는 데이터로 채우기\n",
    "titanic = sns.load_dataset('titanic')\n",
    "#embark_town의 값의 개수를 확인\n",
    "mode = titanic['embark_town'].value_counts()\n",
    "print(mode)\n",
    "print()\n",
    "\n",
    "#결측치를 가장 빈번히 등장하는 단어로 채우기\n",
    "titanic['embark_town'].fillna(mode.idxmax(), inplace=True)\n",
    "print(titanic['embark_town'][825:835])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa660c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b7588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8279bb46",
   "metadata": {},
   "source": [
    "함수형 데이터 - category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f4f87",
   "metadata": {},
   "source": [
    "one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e8e0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           398 non-null    float64\n",
      " 1   cylinders     398 non-null    int64  \n",
      " 2   displacement  398 non-null    float64\n",
      " 3   horsepower    398 non-null    object \n",
      " 4   weight        398 non-null    float64\n",
      " 5   acceleration  398 non-null    float64\n",
      " 6   model year    398 non-null    int64  \n",
      " 7   origin        398 non-null    int64  \n",
      " 8   name          398 non-null    object \n",
      "dtypes: float64(4), int64(3), object(2)\n",
      "memory usage: 28.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 392 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           392 non-null    float64\n",
      " 1   cylinders     392 non-null    int64  \n",
      " 2   displacement  392 non-null    float64\n",
      " 3   horsepower    392 non-null    float64\n",
      " 4   weight        392 non-null    float64\n",
      " 5   acceleration  392 non-null    float64\n",
      " 6   model year    392 non-null    int64  \n",
      " 7   origin        392 non-null    int64  \n",
      " 8   name          392 non-null    object \n",
      "dtypes: float64(5), int64(3), object(1)\n",
      "memory usage: 30.6+ KB\n",
      "[ 46.         107.33333333 168.66666667 230.        ]\n",
      "\n",
      "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
      "0    18.0          8         307.0       130.0  3504.0          12.0   \n",
      "1    15.0          8         350.0       165.0  3693.0          11.5   \n",
      "2    18.0          8         318.0       150.0  3436.0          11.0   \n",
      "3    16.0          8         304.0       150.0  3433.0          12.0   \n",
      "4    17.0          8         302.0       140.0  3449.0          10.5   \n",
      "..    ...        ...           ...         ...     ...           ...   \n",
      "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
      "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
      "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
      "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
      "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
      "\n",
      "     model year  origin                       name hp_bin  \n",
      "0            70       1  chevrolet chevelle malibu   보통출력  \n",
      "1            70       1          buick skylark 320   보통출력  \n",
      "2            70       1         plymouth satellite   보통출력  \n",
      "3            70       1              amc rebel sst   보통출력  \n",
      "4            70       1                ford torino   보통출력  \n",
      "..          ...     ...                        ...    ...  \n",
      "393          82       1            ford mustang gl    저출력  \n",
      "394          82       2                  vw pickup    저출력  \n",
      "395          82       1              dodge rampage    저출력  \n",
      "396          82       1                ford ranger    저출력  \n",
      "397          82       1                 chevy s-10    저출력  \n",
      "\n",
      "[392 rows x 10 columns]\n",
      "     저출력  보통출력  고출력\n",
      "0      0     1    0\n",
      "1      0     1    0\n",
      "2      0     1    0\n",
      "3      0     1    0\n",
      "4      0     1    0\n",
      "..   ...   ...  ...\n",
      "393    1     0    0\n",
      "394    1     0    0\n",
      "395    1     0    0\n",
      "396    1     0    0\n",
      "397    1     0    0\n",
      "\n",
      "[392 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "mpg = pd.read_csv('c://programming/python/pandasdata/auto-mpg.csv', header=None)\n",
    "\n",
    "#열이름 설정\n",
    "mpg.columns = ['mpg', 'cylinders', 'displacement', 'horsepower','weight', \n",
    "               'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "mpg.info()\n",
    "\n",
    "#horsepower를 3개 구간으로 분할\n",
    "\n",
    "#실수로 변환하기 위해서 ?를 None으로 치환\n",
    "mpg['horsepower'].replace('?', np.nan, inplace=True)\n",
    "\n",
    "#horsepower가 None인 행을 삭제\n",
    "mpg.dropna(subset=['horsepower'], axis=0, inplace= True)\n",
    "\n",
    "#실수로 변환\n",
    "mpg['horsepower'] = mpg['horsepower'].astype('float')\n",
    "mpg.info()\n",
    "\n",
    "\n",
    "#구간의 이름을 생성\n",
    "bin_names=['저출력', '보통출력', '고출력']\n",
    "#구간의 경계값을 생성\n",
    "count, bin_dividers = np.histogram(mpg['horsepower'], bins=3)\n",
    "print(bin_dividers)\n",
    "print()\n",
    "\n",
    "#구간 분할\n",
    "mpg['hp_bin'] = pd.cut(x=mpg['horsepower'],\n",
    "                      bins=bin_dividers,\n",
    "                      labels=bin_names,\n",
    "                      include_lowest=True)\n",
    "\n",
    "print(mpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de6d6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     저출력  보통출력  고출력\n",
      "0      0     1    0\n",
      "1      0     1    0\n",
      "2      0     1    0\n",
      "3      0     1    0\n",
      "4      0     1    0\n",
      "..   ...   ...  ...\n",
      "393    1     0    0\n",
      "394    1     0    0\n",
      "395    1     0    0\n",
      "396    1     0    0\n",
      "397    1     0    0\n",
      "\n",
      "[392 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#onehotencoding\n",
    "#범주형 데이터의 개수만큼 컬럼이 생성되고 자신의 값에 해당하는 컬럼에만 1을 대입하고 나머지는 0\n",
    "horsepower_dummies = pd.get_dummies(mpg['hp_bin'])\n",
    "print(horsepower_dummies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63831241",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_18104/2005028362.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\90x61\\AppData\\Local\\Temp/ipykernel_18104/2005028362.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print(one_hot.fit_transform(mpg['hp_bin'])\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# 원-핫 인코더 생성\n",
    "one_hot = preprocessing.LabelBinarizer()\n",
    "# 특성을 원-핫 인코딩\n",
    "print(one_hot.fit_transform(mpg['hp_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "987fbe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 0 0]\n",
      " [0 1 1 0]\n",
      " [0 0 1 1]]\n",
      "['C++' 'Java' 'Javascript' 'Python']\n"
     ]
    }
   ],
   "source": [
    "#여러 개의 특성을 원 핫 인코딩\n",
    "multiclass_feature = [(\"Python\", \"Java\"),\n",
    "                     (\"C++\", \"Python\"),\n",
    "                     (\"C++\", \"Java\"),\n",
    "                     (\"Java\", \"Javascript\"),\n",
    "                     (\"Python\", \"Javascript\")]\n",
    "\n",
    "one_hot_multiclass = preprocessing.MultiLabelBinarizer()\n",
    "print(one_hot_multiclass.fit_transform(multiclass_feature))\n",
    "print(one_hot_multiclass.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7874c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 0 1 1 0 2 2 2 2 2 2 2 2 2 1 2 0 0 0 0 2 2 2 2 2 2 2 2\n",
      " 1 0 1 1 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 0 1 1 1 0 1 1 0 2 1 1 1\n",
      " 1 1 2 2 2 2 2 2 2 2 0 1 1 1 1 0 1 1 1 0 0 0 2 2 2 2 2 2 1 1 0 0 2 2 2 2 2\n",
      " 2 2 2 1 0 2 2 2 1 1 1 1 0 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 0 1 1 1 1 2 1 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 1 1 0 1 1 1 2 2 2 2 2 1 1 1\n",
      " 1 1 2 2 2 0 0 0 1 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 1\n",
      " 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 1 2 2 2 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#하나의 컬럼에 일련번호 형태로 원 핫 인코딩\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "print(label_encoder.fit_transform(mpg['hp_bin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79813a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 5)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (3, 5)\t1.0\n",
      "[[0. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#희소행렬로 표현 - 0이 아주 많은 행렬이라서 0이 아닌 좌표만 가지고\n",
    "features = [[\"Java\", 1], [\"C++\", 2], [\"C#\", 1], [\"Python\", 2]]\n",
    "\n",
    "onehot_encoder = preprocessing.OneHotEncoder()\n",
    "print(onehot_encoder.fit_transform(features))\n",
    "\n",
    "#밀집행렬로 표현 - 원 핫 인코딩과 유사한 결과\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "print(onehot_encoder.fit_transform(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bfdff9",
   "metadata": {},
   "source": [
    "순서가 있는 경우의 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61ed4a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   점수\n",
      "0  저조\n",
      "1  보통\n",
      "2  우수\n",
      "3  보통\n",
      "4  저조\n",
      "\n",
      "   점수  encoder\n",
      "0  저조        1\n",
      "1  보통        2\n",
      "2  우수        3\n",
      "3  보통        2\n",
      "4  저조        1\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"점수\":[\"저조\", \"보통\", \"우수\", \"보통\", \"저조\"]})\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "scale_mapper = {\"저조\":1, \"보통\":2, \"우수\":3}\n",
    "\n",
    "df['encoder'] = df['점수'].replace(scale_mapper)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8827f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 2.]\n",
      " [2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#문자열이나 숫자(문자로 취급)를 정렬해서 인코딩\n",
    "features = np.array([[\"Low\", 10],\n",
    "                    [\"High\", 3],\n",
    "                    [\"Medium\", 27]])\n",
    "\n",
    "ordinal_encoder = preprocessing.OrdinalEncoder()\n",
    "print(ordinal_encoder.fit_transform(features))\n",
    "#알파벳 순서, 숫자는 크기가 아닌 맨 앞자리의 숫자 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894d892",
   "metadata": {},
   "source": [
    "Dictionary를 특성 행렬로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "019f578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 2. 0.]\n",
      " [3. 4. 0.]\n",
      " [0. 1. 2.]\n",
      " [0. 2. 2.]]\n",
      "\n",
      "['Blue', 'Red', 'Yellow']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "      <th>Yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red  Yellow\n",
       "0   4.0  2.0     0.0\n",
       "1   3.0  4.0     0.0\n",
       "2   0.0  1.0     2.0\n",
       "3   0.0  2.0     2.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라이브러리를 임포트합니다.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 딕셔너리를 만듭니다.\n",
    "data_dict = [{\"Red\": 2, \"Blue\": 4},\n",
    "             {\"Red\": 4, \"Blue\": 3},\n",
    "             {\"Red\": 1, \"Yellow\": 2},\n",
    "             {\"Red\": 2, \"Yellow\": 2}]\n",
    "\n",
    "# DictVectorizer 객체를 만듭니다.\n",
    "dictVectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# 딕셔너리를 특성 행렬로 변환합니다.\n",
    "features = dictVectorizer.fit_transform(data_dict)\n",
    "print(features)\n",
    "print()\n",
    "\n",
    "# 특성 이름을 얻습니다.\n",
    "feature_names = dictVectorizer.get_feature_names()\n",
    "print(feature_names)\n",
    "print()\n",
    "\n",
    "# 특성으로 데이터프레임을 만듭니다.\n",
    "pd.DataFrame(features, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d5bd2",
   "metadata": {},
   "source": [
    "결측치 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3fd72",
   "metadata": {},
   "source": [
    "머신러닝 이용 권장하나, \n",
    "데이터가 많은 경우 알고리즘 수행시간이 많이 소요되므로 자주 등장하는 값으로 대체하는 경우 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e6143f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#두 가지의 행렬 생성\n",
    "#X와 같은 대문자의 값은 훈련 데이터\n",
    "\n",
    "# 범주형 특성을 가진 특성 행렬 생성\n",
    "X = np.array([[0, 2.10, 1.45],\n",
    "              [1, 1.18, 1.33],\n",
    "              [0, 1.22, 1.27],\n",
    "              [1, -0.21, -1.19]])\n",
    "\n",
    "# 범주형 특성에 누락된 값이 있는 특성 행렬 생성\n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
    "                       [np.nan, -0.67, -0.22]])\n",
    "\n",
    "# KNN 학습기를 훈련\n",
    "clf = KNeighborsClassifier(3, weights='distance')\n",
    "#1번열 이후의 데이터를 가지고 0번 열을 예측하는 모델을 생성\n",
    "trained_model = clf.fit(X[:,1:], X[:,0])\n",
    "# 누락된 값의 클래스를 예측\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
    "print(imputed_values)\n",
    "print()\n",
    "\n",
    "\n",
    "# 예측된 클래스와 원본 특성을 열로 합침\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n",
    "\n",
    "# 두 특성 행렬을 연결\n",
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "defeb98d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Imputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18104/2207935117.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#가장 자주 등장하는 데이터로 누락된 클래스 대체\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'most_frequent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_complete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Imputer' is not defined"
     ]
    }
   ],
   "source": [
    "# 두 개의 특성 행렬을 합침\n",
    "X_complete = np.vstack((X_with_nan, X))\n",
    "\n",
    "#가장 자주 등장하는 데이터로 누락된 클래스 대체\n",
    "imputer = Imputer(strategy='most_frequent', axis=0)\n",
    "imputer.fit_transform(X_complete)\n",
    "\n",
    "\n",
    "# 두 개의 특성 행렬을 합침\n",
    "X_complete = np.vstack((X_with_nan, X))\n",
    "#가장 자주 등장하는 데이터로 누락된 클래스 대체\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b925c18",
   "metadata": {},
   "source": [
    "불균형한 데이터 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae85215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(class_weight={0: 0.9, 1: 0.1})\n",
      "\n",
      "RandomForestClassifier(class_weight='balanced')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#분류기를 만들 때 가중치를 적용\n",
    "weights = {0:.9, 1:.1}\n",
    "rfc = RandomForestClassifier(class_weight=weights)\n",
    "print(rfc)\n",
    "print()\n",
    "\n",
    "\n",
    "# 균형잡힌 클래스 가중치 적용\n",
    "rfc = RandomForestClassifier(class_weight=\"balanced\")\n",
    "print(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0619ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#0이 10개이고 1이 100개인 불균형한 데이터 만들기\n",
    "list1 = []\n",
    "list2 = []\n",
    "for i in range(0,10,1):\n",
    "    list1.append(0)\n",
    "for i in range(0,100,1):\n",
    "    list2.append(1)\n",
    "target = np.array(list1+list2)\n",
    "print(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7a8bb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27\n",
      "  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45\n",
      "  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63\n",
      "  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81\n",
      "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109]\n",
      "[ 85  46  97  10  68  37  36  91 107  27]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0번과 1번의 인덱스를 추출\n",
    "i_class0 = np.where(target == 0)[0]\n",
    "i_class1 = np.where(target == 1)[0]\n",
    "print(i_class0)\n",
    "print(i_class1)\n",
    "\n",
    "#데이터의 개수 확인\n",
    "n_class0 = len(i_class0)\n",
    "n_class1 = len(i_class1)\n",
    "\n",
    "#다운 샘플링 - 랜덤하게 추출\n",
    "i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace = False)\n",
    "print(i_class1_downsampled)\n",
    "print()\n",
    "np.hstack((target[i_class0], target[i_class1_downsampled]))\n",
    "\n",
    "#업 샘플링\n",
    "i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace = True)\n",
    "np.hstack((target[i_class0_upsampled], target[i_class1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba24cd7",
   "metadata": {},
   "source": [
    "텍스트 마이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67f67b",
   "metadata": {},
   "source": [
    "파이썬의 str클래스에서 제공하는 함수를 이용한 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed4ce037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Hyeryun.', '     .I am a programmer', 'We are the one.      ']\n",
      "\n",
      "['My name is Hyeryun.', '.I am a programmer', 'We are the one.']\n",
      "\n",
      "['MY NAME IS HYERYUN.', '     .I AM A PROGRAMMER', 'WE ARE THE ONE.      ']\n",
      "\n",
      "['MY NAME IS HYERYUN.', '     .I AM A PROGRAMMER', 'WE ARE THE ONE.      ']\n",
      "\n",
      "['my name is hyeryun.', '     .i am a programmer', 'we are the one.      ']\n",
      "\n",
      "['My name is Hyeryun', '     I am a programmer', 'We are the one      ']\n",
      "\n",
      "['MynameisHyeryun.', '.Iamaprogrammer', 'Wearetheone.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data = ['My name is Hyeryun.',\n",
    "            '     .I am a programmer',\n",
    "            'We are the one.      ']\n",
    "print(text_data)\n",
    "print()\n",
    "\n",
    "#좌우공백 제거\n",
    "strip_text = [string.strip() for string in text_data]\n",
    "print(strip_text)\n",
    "print()\n",
    "\n",
    "#모두 대문자로 만들기\n",
    "upper_text = [string.upper() for string in text_data]\n",
    "print(upper_text)\n",
    "print()\n",
    "\n",
    "#모두 대문자로 만들기 - 함수를 이용\n",
    "def capitalizer(string:str) -> str:\n",
    "    return string.upper()\n",
    "print([capitalizer(string) for string in text_data])\n",
    "print()\n",
    "\n",
    "#모두 소문자로 만들기\n",
    "lower_text = [string.lower() for string in text_data]\n",
    "print(lower_text)\n",
    "print()\n",
    "\n",
    "#마침표 제거\n",
    "remove_periods = [string.replace('.', '') for string in text_data]\n",
    "print(remove_periods)\n",
    "print()\n",
    "\n",
    "#모든 공백 제거\n",
    "remove_periods = [string.replace(' ', '') for string in text_data]\n",
    "print(remove_periods)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b348b88",
   "metadata": {},
   "source": [
    "텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e130578",
   "metadata": {},
   "source": [
    "1)횟수 관련 메타 문자\n",
    "*: 0회 이상의 반복\n",
    "\n",
    "+: 1회 이상의 반복\n",
    "\n",
    "?\" 0 번이나 1회\n",
    "\n",
    "{정수}: 정수 만큼 반복\n",
    "\n",
    "{정수1, 정수2}: 정수1 에서 정수2만큼 반복\n",
    "\n",
    "{정수, }: 정수 이상 반복\n",
    "\n",
    "\n",
    "a{3, 5}: a를 3번에서 5번\n",
    "a{3,}: a를 3번 이상\n",
    "\n",
    "2)매칭 관련 메타 문자\n",
    ".: 줄바꿈을 제외한 1개 문자\n",
    "\n",
    "^: 시작을 나타내는 기호, [ ] 안에서 사용되면 제외\n",
    "    ^a - a로 시작하는 , [^a] - a를 제외한\n",
    "\n",
    "$: 끝을 나타내는 기호, [ ]안에서 사용되면 $\n",
    "\n",
    "[ ]: 문자 집합으로 ,를 이용해서 구분할 수 있고 -를 이용해서 범위를 설정할 수 있습니다.\n",
    "     [ac] - a 또는 c \n",
    "     [a-c] - a에서 c까지\n",
    "\n",
    "|: 또는\t\n",
    "\n",
    "( ): 정규식을 그룹으로 묶기\t\n",
    "\n",
    "\n",
    "3)특수 문자\n",
    "\\\\: \\\n",
    "\n",
    "\\d: 숫자\n",
    "\n",
    "\\D: 숫자를 제외한\n",
    "\n",
    "\\s: 화이트 스페이스(공백이나 엔터와 같은 제어문자)\n",
    "\n",
    "\\S: 화이트 스페이스를 제외한\n",
    "\n",
    "\\w: 글자(숫자 나 문자)\n",
    "\n",
    "\\W: 글자를 제외한\n",
    "\n",
    "\\b: 단어의 경계\n",
    "\n",
    "\\B: 단어의 경계를 제외한\n",
    "\n",
    "4)플래그\n",
    "re.I: 대소문자 구분하지 않음\n",
    "\n",
    "re.M: 여러 줄에 걸쳐서 찾음\n",
    "\n",
    "5)정규식은 문자열 클래스 와 정규식 관련 클래스를 통해 사용\n",
    "=>문자열 클래스의 메서드를 확인할 때 regexp 나 reg 또는 pattern 이라는 단어가 들어가면 정규 표현식을 의미합니다.\n",
    "\n",
    "6)re 모듈의 함수\n",
    "=>compile(pattern[, flags]): 정규식 객체를 생성\n",
    "=>search\n",
    "=>match\n",
    "=>split\n",
    "=>findall\n",
    "=>sub: 치환\n",
    "\n",
    "7)Match 인스턴스의 함수\n",
    "=>group(): 찾은 문자열을 반환\n",
    "=>groups(): 매칭된 전체 그룹 문자열을 튜플로 반환\n",
    "=>start(): 매칭된 문자열의 시작 위치를 리턴\n",
    "=>end(): 매칭된 문자열의 끝 위치를 리턴\n",
    "=>span: 시작 위치 와 끝 위치를 튜플로 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da1693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('\\\\W+')\n",
      " 안녕하세요 반갑습니다 112312321\n",
      "re.compile('\\\\d+')\n",
      " 안녕하세요 반갑습니다  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 특수 문자 와 숫자 제거\n",
    "string = \"★안녕하세요 반갑습니다. 112312321\"\n",
    "\n",
    "#특수 문자 제거\n",
    "p = re.compile(\"\\W+\") #단어가 아닌 - 숫자 나 문자가 아닌\n",
    "print(p)\n",
    "result = p.sub(\" \", string)\n",
    "print(result)\n",
    "\n",
    "#숫자 제거\n",
    "p = re.compile(\"\\d+\") #숫자\n",
    "result = p.sub(\" \", result)\n",
    "print(p)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04644610",
   "metadata": {},
   "source": [
    "텍스트 전처리 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd81b07",
   "metadata": {},
   "source": [
    "1)nltk 패키지\n",
    "=>자연어 처리 와 관련된 여러 알고리즘을 구현한 패키지 - 자연어 처리에서는 필수\n",
    "=>한 개의 패키지로 구성되어 있지 않고 여러 개의 패키지로 구성되어 있습니다.\n",
    "=>설치\n",
    "pip install nltk\n",
    "\n",
    "상황에 따라서 pip를 업그레이드 하라는 메시지가 출력되기도 하는데 이 경우는 메시지 대로 명령어를 수행하면 됩니다.\n",
    " \n",
    "2)nltk 안의 punkt\n",
    "=>토큰화를 위한 패키지\n",
    "=>단어 단위나 문장 단위로 분할해주는 기능을 토큰화라고 합니다.\n",
    "=>파이썬 코드로 설치\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "3)nltk 의 stopwords(불용어)\n",
    "=>불용어를 위한 패키지\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "4)토큰화\n",
    "=>단어 단위나 문장 단위로 분할하는 작업\n",
    "=>nltk 패키지의 tokenize 의 word_tokenize 나 sent_tokenize 함수를 이용\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb63e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\90x61\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\90x61\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#토큰화 패키지 - 단어, 문장단위 분할\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#불용어\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba08142",
   "metadata": {},
   "source": [
    "토큰화\n",
    "=>단어 단위나 문장 단위로 분할하는 작업\n",
    "=>nltk 패키지의 tokenize 의 word_tokenize 나 sent_tokenize 함수를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a3f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am a boy.', 'You are a girl.']\n",
      "['I', 'am', 'a', 'boy', '.', 'You', 'are', 'a', 'girl', '.']\n"
     ]
    }
   ],
   "source": [
    "#문장 토큰화\n",
    "string = \"I am a boy. You are a girl.\"\n",
    "#구두점을 기준으로 분할해서 list로 리턴\n",
    "print(sent_tokenize(string))\n",
    "\n",
    "#단어 토큰화 - 공백을 기준으로 분할해서 list로 리턴\n",
    "print(word_tokenize(string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264305df",
   "metadata": {},
   "source": [
    "불용어\n",
    "=>텍스트 데이터에서 특별한 의미를 부여하기 힘든 단어\n",
    "=>영어의 경우는 nltk 패키지 와 sklearn 에서 불용어 리스트를 제공\n",
    "=>nltk 의 경우는 다른 주요 언어(유럽)도 제공\n",
    "=>한국어는 제공되지 않기 때문에 직접 작성\n",
    "=>한국어 불용어 리스트를 제공해주는 사이트\n",
    "https://www.ranks.nl/stopwords/korean\n",
    "https://github.com/6/stopwords-json/blob/master/dist/ko.json\n",
    "https://github.com/stopwords-iso/stopwords-ko/blob/master/stopwords-ko.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7cfeb",
   "metadata": {},
   "source": [
    "corpus\n",
    "=>분석 대상이 되는 글자의 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89fa133",
   "metadata": {},
   "source": [
    "불용어(stopword - 의미없는 단어) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a2b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'boy', 'movie']\n",
      "\n",
      "['I', 'boy', 'movie']\n",
      "\n",
      "['이번', '주말에', '서점에', '강화학습', '책을', '볼', '예정입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#영어 불용어 처리\n",
    "word_english = ['I', \"am\", \"a\", \"boy\", \"and\", \"you\", \"movie\"]\n",
    "\n",
    "#word_english 의 모든 내용을 w에 대입하고\n",
    "#w 가 stopwords.word('english')에 속하지 않은 경우만 가지고\n",
    "#list를 생성\n",
    "result = [w for w in word_english \n",
    "              if not w in stopwords.words('english')]\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "result = [w for w in word_english \n",
    "              if not w in ENGLISH_STOP_WORDS]\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "\n",
    "#한글 불용어 처리\n",
    "sentence = \"나는 이번 주말에 서점에 가서 강화학습 책을 볼 예정입니다.\" \n",
    "\n",
    "#불용어 사전\n",
    "stopwords_kor = ['나는', '가서']\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#문장을 단어 단위로 토큰화\n",
    "words_kor = word_tokenize(sentence)\n",
    "\n",
    "result = [i for i in words_kor if i not in stopwords_kor]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ea058",
   "metadata": {},
   "source": [
    "7)어간 추출\n",
    "=>스펠이 다르더라도 의미가 같은 단어를 추출하는 것을 어간 추출\n",
    "=>한국어는 어미, 조사에 따라 단어의 형태가 바뀌고 영어의 경우 주어의 형태, 시제에 따라 동사의 형태가 바뀌고 단수 냐 복수 냐에 따라 s 나 es 가 붙음\n",
    "=>어간 추출을 하지 않고 텍스트 분석을 같은 의미를 갖는 단어인데 스펠이 달라서 다른 단어로 인식을 하게 되면 잘못된 분석 결과를 만들 수 있습니다.\n",
    "=>영문의 경우는 NLTK 패키지의 PorterStemmer 나 LancasterStemmer 라이브러리를 이용해서 할 수 있고 한국어는 konlpy 와 같은 형태소 분석기를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5442cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pythoners have pythoned at least once\n",
      "['All', 'pythoners', 'have', 'pythoned', 'at', 'least', 'once']\n",
      "all python have python at least onc \n",
      "al python hav python at least ont \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = 'All pythoners have pythoned at least once'\n",
    "print(string)\n",
    "\n",
    "#단어 단위로 토큰화\n",
    "words = word_tokenize(string)\n",
    "print(words)\n",
    "\n",
    "#어간 추출\n",
    "ps_stemmer = PorterStemmer()\n",
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=' ')\n",
    "print()\n",
    "\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls_stemmer = LancasterStemmer()\n",
    "for w in words:\n",
    "    print(ls_stemmer.stem(w), end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ce2dd",
   "metadata": {},
   "source": [
    "n-gram\n",
    "=>n 번 이상 연이어 등장하는 단어들의 연속적인 모임\n",
    "=>2개 연속해서 등장하면 바이그램, 3개 연속해서 등장하면 트라이그램\n",
    "=>2개 이상의 단어의 조합이 하나의 의미를 갖는 경우 이를 하나로 처리\n",
    "United Kingdom: 2개를 묶어서 하나의 단어로 판단\n",
    "United States America, Republic Of Korea: 3개를 묶어서 하나의 단어로 판단\n",
    "\n",
    "=>nltk 패키지의 ngrams 를 이용하면 생성할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0f0bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('대한', '민국') ('민국', '우리') ('우리', '나라') ('나라', '대한') ('대한', '민국') \n",
      "('대한', '민국', '우리') ('민국', '우리', '나라') ('우리', '나라', '대한') ('나라', '대한', '민국') "
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"대한 민국 우리 나라 대한 민국\"\n",
    "\n",
    "#2개씩 묶기 - bigram\n",
    "grams = ngrams(word_tokenize(sentence), 2)\n",
    "for i in grams:\n",
    "    print(i, end=' ')\n",
    "print()\n",
    "\n",
    "#3개씩 묶기 - trigram\n",
    "grams = ngrams(word_tokenize(sentence), 3)\n",
    "for i in grams:\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802c5534",
   "metadata": {},
   "source": [
    "영문 형태소 분석 - 품사 태깅, 문장을 의미를 갖는 corpus 로 분할하고 품사를 할당\n",
    "=>영문의 경우는 nltk 패키지의 averaged_perceptron_tagger 를 설치\n",
    "=>함수에 단어의 list를 대입하면 단어 와 품사 태그로 이루어진 튜플의 리스트를 리턴\n",
    "=>품사 태그\n",
    "NNP: 고유명사 등의 형태로 이미 설정되어 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da60dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\90x61\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'boy', 'You', 'are', 'a', 'girl']\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('boy', 'NN'), ('You', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('girl', 'NN')]\n",
      "['boy', 'girl']\n"
     ]
    }
   ],
   "source": [
    "#영문 형태소 분석을 위한 라이브러리 설치\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'I am a boy You are a girl'\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "#품사 태깅\n",
    "tags = pos_tag(tokens)\n",
    "print(tags)\n",
    "\n",
    "## 특정 품사만 골라내기\n",
    "print([word for word, tag in tags \n",
    "       if tag in ['NN', 'NNS', 'NNP', 'NNPS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399237f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "=>원 핫 인코딩: 범주형 데이터의 경우 각 범주를 하나의 열로 만들고\n",
    "    자신의 범주에 해당하는 열에만 1을 설정하고 나머지는 0으로 설정하는 것\n",
    "    자연어 처리에서 유사도 측정을 하기 위해서는 각 단어를 숫자로 변환을 해서 \n",
    "    거리를 계산을 해야 하기 때문에 이 작업을 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b2248",
   "metadata": {},
   "source": [
    "=>영문 형태소 분석의 정확도가 떨어지는 경우 - 자연어는 시간이 흐르면 새로운 단어가 만들어지고 이전의 단어들이 없어기도 합니다.\n",
    "많은 텍스트 문서를 모아서 모델을 계속해서 훈련을 시키는 것이 가장 좋습니다.\n",
    "\n",
    "현실적으로 어려운 문제라서 계속해서 업데이트가 되는 brown corpus 를 활용하거나 Backoff n-gram 모델을 이용해서 다른 단어의 품사를 예측해서 이용\n",
    "\n",
    "브라운 코퍼스에는 카테고리 별로 아주 많은 문장들이 만들어져 있습니다.\n",
    "최근에는 google 의 bert 가 훨씬 더 문장을 소유하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ab961eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'NN', 'PRP', 'VBP', 'DT', 'RBR', 'JJ', 'PRP', 'MD', 'VB', 'DT', 'NN']\n",
      "['NNP', 'IN', 'PRP']\n",
      "['DT', 'NN', 'RB', 'NN']\n",
      "['DT', 'NN', 'NNP', 'NN']\n",
      "['NN', 'VBZ', 'RB', 'JJ', 'WRB', 'PRP', 'VBZ', 'TO', 'NN']\n",
      "\n",
      "[[1 0 1 1 1 0 1 0 1 0 1 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 1 1 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#품사 태깅 후 원 핫 인코딩\n",
    "maxims = [\"The harder you work the more likely you can reach the goal\",\n",
    "         \"Believe in yourself\",\n",
    "         \"No pain No gain\",\n",
    "         \"No sweat No sweet\",\n",
    "         \"Courage is very important when it comes to anything\"]\n",
    "\n",
    "#품사 태깅한 내용을 저장할 list\n",
    "tagged_maxims = []\n",
    "for maxim in maxims:\n",
    "    maxim_tag = nltk.pos_tag(word_tokenize(maxim))\n",
    "    tagged_maxims.append([tag for word, tag in maxim_tag])\n",
    "    \n",
    "#품사 확인\n",
    "for temp in tagged_maxims:\n",
    "    print(temp)\n",
    "print()\n",
    "\n",
    "#여러 개의 열에 1을 설정할 수 있는 One Hot Encoding\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#각 문장이 어떤 품사들로 구성되어 있는지 확인 가능\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "print(one_hot_multi.fit_transform(tagged_maxims))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea989c",
   "metadata": {},
   "source": [
    "=>영문 형태소 분석의 정확도가 떨어지는 경우 - 자연어는 시간이 흐르면 새로운 단어가 만들어지고 이전의 단어들이 없어기도 합니다.\n",
    "많은 텍스트 문서를 모아서 모델을 계속해서 훈련을 시키는 것이 가장 좋습니다.\n",
    "\n",
    "현실적으로 어려운 문제라서 계속해서 업데이트가 되는 brown corpus 를 활용하거나 Backoff n-gram 모델을 이용해서 다른 단어의 품사를 예측해서 이용\n",
    "\n",
    "브라운 코퍼스에는 카테고리 별로 아주 많은 문장들이 만들어져 있습니다.\n",
    "최근에는 google 의 bert 가 훨씬 더 문장을 소유하고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2b841",
   "metadata": {},
   "source": [
    "BoW(Bag of Word) - 단어의 등장 횟수\n",
    "=>텍스트 데이터에서 특정 단어의 등장 횟수를 나타내는 특성을 만드는 작업\n",
    "단어의 중요도를 파악하기 위해서 필요한 작업\n",
    "=>sklearn 의 CountVectorizer 객체를 이용하면 BoW 행렬을 생성할 수 있습니다.\n",
    "=>객체를 생성할 때 ngram_range 옵션을 이용해서 ngram 지정 가능\n",
    "=>stop_words 를 이용해서 불용어 제거 가능\n",
    "=>vocabulary 옵션을 이용해서 특정 단어의 개수만 세는 것도 가능\n",
    "=>max_df 에 단어가 등장할 문서의 최대 개수를 설정\n",
    "=>min_df 에 단어가 등장할 문서의 최소 개수를 설정\n",
    "=>max_feature 속성을 이용해서 상위 몇개의 단어만 추출\n",
    "\n",
    "=>get_feature_names 속성을 이용하면 각 열에 연결된 단어를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5411d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "\n",
      "[[0 0 0 0 0 1 2 0]\n",
      " [0 1 0 0 1 0 0 1]\n",
      " [1 0 1 1 0 0 0 0]]\n",
      "\n",
      "['beats', 'best', 'both', 'germany', 'is', 'love', 'newziland', 'sweden']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#테스트 데이터 만들기\n",
    "text_data = np.array([\n",
    "    'I love newziland. newziland', \n",
    "    'Sweden is best',\n",
    "    'Germany beats both'\n",
    "])\n",
    "\n",
    "#BoW 특성 행렬 생성\n",
    "count = CountVectorizer()\n",
    "#희소행렬을 생성\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(bag_of_words)\n",
    "print()\n",
    "\n",
    "#밀집행렬을 출력\n",
    "print(bag_of_words.toarray())\n",
    "print()\n",
    "\n",
    "#각 열의 이름을 확인\n",
    "print(count.get_feature_names())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "단어의 가중치 부여\n",
    "=>tf-idf(term frequency - inverse document frequency)\n",
    "tf: 하나의 단어가 하나의 문서에서 여러 번 등장하면 중요도가 높다.\n",
    "idf: 하나의 단어가 여러 문서에서 여러 번 등장하면 중요도가 낮다.\n",
    "=>sklearn 의 TfidfVectorizer 클래스가 계산을 수행해 줌\n",
    "=>계산 방식\n",
    "log((1+문서의개수)/1 + 단어t 의 문서 빈도) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6d7a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.8444932017012523\n",
      "  (0, 6)\t0.5355662725381126\n",
      "  (1, 1)\t0.6176143709756019\n",
      "  (1, 4)\t0.48693426407352264\n",
      "  (1, 9)\t0.6176143709756019\n",
      "  (2, 2)\t0.5773502691896257\n",
      "  (2, 0)\t0.5773502691896257\n",
      "  (2, 3)\t0.5773502691896257\n",
      "  (3, 10)\t0.2747918015856176\n",
      "  (3, 8)\t0.2747918015856176\n",
      "  (3, 11)\t0.8243754047568529\n",
      "  (3, 5)\t0.2747918015856176\n",
      "  (3, 4)\t0.21664901266330147\n",
      "  (3, 7)\t0.21664901266330147\n",
      "\n",
      "{'love': 6, 'newziland': 7, 'sweden': 9, 'is': 4, 'best': 1, 'germany': 3, 'beats': 0, 'both': 2, 'korea': 5, 'top': 11, 'of': 8, 'the': 10}\n",
      "\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.53556627 0.8444932  0.         0.         0.         0.        ]\n",
      " [0.         0.61761437 0.         0.         0.48693426 0.\n",
      "  0.         0.         0.         0.61761437 0.         0.        ]\n",
      " [0.57735027 0.         0.57735027 0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21664901 0.2747918\n",
      "  0.         0.21664901 0.2747918  0.         0.2747918  0.8243754 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tf-idf : 단어의 가중치 부여\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#테스트 데이터 만들기\n",
    "text_data = np.array([\n",
    "    'I love newziland. newziland', \n",
    "    'Sweden is best',\n",
    "    'Germany beats both',\n",
    "    'Korea newziland is top of the top top'\n",
    "])\n",
    "\n",
    "#단어의 중요도를 희소행렬로 출력\n",
    "#하나의 문장에서 자주 등장하면 중요도는 높아짐\n",
    "#여러 문장에서 자주 등장하면 중요도는 낮아짐\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "print(feature_matrix)\n",
    "print()\n",
    "\n",
    "#특성의 이름 파악 \n",
    "print(tfidf.vocabulary_)\n",
    "print()\n",
    "\n",
    "#밀집 행렬로 변환\n",
    "print(feature_matrix.toarray())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f498243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a567f1",
   "metadata": {},
   "source": [
    "시계열 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fda0b4",
   "metadata": {},
   "source": [
    "1.시계열 데이터\n",
    "=>날짜 와 시간을 이용해서 정렬된 데이터\n",
    "1)제공되는 사용할 수 있는 데이터 셋\n",
    "=> UCI 머신러닝 저장소\n",
    "  - https://perma.cc/56Q5-YPNT\n",
    "  - https://perma.cc/U6MU-2SCZ : 요가 수행자의 성별에 따른 요가 동작의 변환 데이터\n",
    "  - https://perma.cc/Y34R-UGMD :  와인 데이터 셋으로 시간 값은 없지만 일정한 패턴을 갖는 형태의 데이터로 시계열 분석에서 사용할 수 있음\n",
    "\n",
    "=>정부 시계열 데이터 셋\n",
    "\n",
    "=>CompEngine - https://arxiv.org/abs/1905.01042\n",
    "\n",
    "2)발견된 시계열 데이터\n",
    "=>타임스탬프가 찍힌 이벤트 기록 - 항해 일지, 항공 운항 기록 등\n",
    "=>타임스탬프는 없지만 일정한 간격을 갖는 데이터 - 동영상에서의 이미지\n",
    "=>물리적 흔적: 의학, 청각학, 기상학 등에서의 \n",
    "\n",
    "3)시계열 자료형\n",
    "=>pandas 에서는 datetime64(TimeStamp) 과 Period(간격) 자료형 사용\n",
    "=>문자열을 datetime64로 변환: pandas.to_datetime\n",
    "  - format 매개변수에 날짜 서식을 설정\n",
    "  - errors 매개변수에 ignore를 입력하면 에러를 무시하고 원본 문자열을 리턴하고 coerce를 에러가 발생하면 멈추지는 않지만 에러가 난 값을 NaN(NaT) 으로 설정하고 raise 로 설정하면 에러가 발생하면 예외를 발생시킴\n",
    "=>날짜 와 시간 포맷\n",
    "  %Y: 연도\n",
    "  %m: 월\n",
    "  %d: 일\n",
    "  %l: 시간\n",
    "  %p: AM 또는 PM\n",
    "  %M: 분\n",
    "  %S: 초\n",
    "\n",
    "=>변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dba85c",
   "metadata": {},
   "source": [
    "문자열을 시계열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e1cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    20 non-null     object\n",
      " 1   Close   20 non-null     int64 \n",
      " 2   Start   20 non-null     int64 \n",
      " 3   High    20 non-null     int64 \n",
      " 4   Low     20 non-null     int64 \n",
      " 5   Volume  20 non-null     int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   Date     20 non-null     object        \n",
      " 1   Close    20 non-null     int64         \n",
      " 2   Start    20 non-null     int64         \n",
      " 3   High     20 non-null     int64         \n",
      " 4   Low      20 non-null     int64         \n",
      " 5   Volume   20 non-null     int64         \n",
      " 6   NewDate  20 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5), object(1)\n",
      "memory usage: 1.2+ KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stock = pd.read_csv('c://programming/python/pandasdata/stock-data.csv')\n",
    "\n",
    "#첫번째 열인 Date의 자료형이 object\n",
    "stock.head()\n",
    "stock.info()\n",
    "print()\n",
    "\n",
    "stock['NewDate'] = pd.to_datetime(stock['Date'])\n",
    "stock.info()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad0b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f5db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7de427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906cef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9197987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d973d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b94b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
