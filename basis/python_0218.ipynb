{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727192be",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc71114",
   "metadata": {},
   "source": [
    "## 텍스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf0b4e",
   "metadata": {},
   "source": [
    "### 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636735d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 안녕하세요 반갑습니다 112312321\n",
      " 안녕하세요 반갑습니다  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 특수 문자 와 숫자 제거\n",
    "string = \"★안녕하세요 반갑습니다. 112312321\"\n",
    "\n",
    "#특수 문자 제거\n",
    "p = re.compile(\"\\W+\") #단어가 아닌 - 숫자 나 문자가 아닌\n",
    "result = p.sub(\" \", string)\n",
    "print(result)\n",
    "\n",
    "#숫자 제거\n",
    "p = re.compile(\"\\d+\") #숫자\n",
    "result = p.sub(\" \", result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea8572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926e99ff",
   "metadata": {},
   "source": [
    "## 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50858e96",
   "metadata": {},
   "source": [
    "### 필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78f44e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963a1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b58c8a",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041d625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am a boy.', 'You are a girl.']\n",
      "['I', 'am', 'a', 'boy', '.', 'You', 'are', 'a', 'girl', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#문장 토큰화\n",
    "string = \"I am a boy. You are a girl.\"\n",
    "#구두점을 기준으로 분할해서 list로 리턴\n",
    "print(sent_tokenize(string))\n",
    "\n",
    "#단어 토큰화 - 공백을 기준으로 분할해서 list로 리턴\n",
    "print(word_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26191a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "962bf4c0",
   "metadata": {},
   "source": [
    "### 불용어(stopword - 의미없는 단어) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284a9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'boy', 'movie']\n",
      "['I', 'boy', 'movie']\n",
      "['이번', '주말에', '서점에', '강화학습', '책을', '볼', '예정입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "word_english = ['I', \"am\", \"a\", \"boy\", \"and\", \"you\", \"movie\"]\n",
    "#word_english 의 모든 내용을 w에 대입하고\n",
    "#w 가 stopwords.word('englisht')에 속하지 않은 경우만 가지고\n",
    "#list를 생성\n",
    "result = [w for w in word_english \n",
    "              if not w in stopwords.words('english')]\n",
    "print(result)\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "result = [w for w in word_english \n",
    "              if not w in ENGLISH_STOP_WORDS]\n",
    "print(result)\n",
    "\n",
    "#한글 불용어 처리\n",
    "sentence = \"나는 이번 주말에 서점에 가서 강화학습 책을 볼 예정입니다.\" \n",
    "\n",
    "#불용어 사전\n",
    "stopwords_kor = ['나는', '가서']\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#문장을 단어 단위로 토큰화\n",
    "words_kor = word_tokenize(sentence)\n",
    "\n",
    "result = [i for i in words_kor if i not in stopwords_kor]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625bc419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aae1449",
   "metadata": {},
   "source": [
    "### 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fddb420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pythoners have pythoned at least once\n",
      "['All', 'pythoners', 'have', 'pythoned', 'at', 'least', 'once']\n",
      "all python have python at least onc \n",
      "al python hav python at least ont \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = 'All pythoners have pythoned at least once'\n",
    "print(string)\n",
    "\n",
    "#단어 단위로 토큰화\n",
    "words = word_tokenize(string)\n",
    "print(words)\n",
    "\n",
    "#어간 추출\n",
    "ps_stemmer = PorterStemmer()\n",
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=' ')\n",
    "print()\n",
    "\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls_stemmer = LancasterStemmer()\n",
    "for w in words:\n",
    "    print(ls_stemmer.stem(w), end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a147ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b30314a",
   "metadata": {},
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bab26ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('대한', '민국') ('민국', '우리') ('우리', '나라') ('나라', '대한') ('대한', '민국') ('대한', '민국', '우리') ('민국', '우리', '나라') ('우리', '나라', '대한') ('나라', '대한', '민국') "
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"대한 민국 우리 나라 대한 민국\"\n",
    "#2개씩 묶기 - bigram\n",
    "grams = ngrams(word_tokenize(sentence), 2)\n",
    "for i in grams:\n",
    "    print(i, end=' ')\n",
    "print()\n",
    "\n",
    "#3개씩 묶기 - trigram\n",
    "grams = ngrams(word_tokenize(sentence), 3)\n",
    "for i in grams:\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1677a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a104c306",
   "metadata": {},
   "source": [
    "### 영문 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c69177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영문 형태소 분석을 위한 라이브러리 설치\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c5da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'boy', 'You', 'are', 'a', 'girl']\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('boy', 'NN'), ('You', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('girl', 'NN')]\n",
      "['boy', 'girl']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'I am a boy You are a girl'\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "#품사 태깅\n",
    "tags = pos_tag(tokens)\n",
    "print(tags)\n",
    "\n",
    "## 특정 품사만 골라내기\n",
    "print([word for word, tag in tags \n",
    "       if tag in ['NN', 'NNS', 'NNP', 'NNPS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d892b0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'NN', 'PRP', 'VBP', 'DT', 'RBR', 'JJ', 'PRP', 'MD', 'VB', 'DT', 'NN']\n",
      "['NNP', 'IN', 'PRP']\n",
      "['DT', 'NN', 'RB', 'NN']\n",
      "['DT', 'NN', 'NNP', 'NN']\n",
      "['NN', 'VBZ', 'RB', 'JJ', 'WRB', 'PRP', 'VBZ', 'TO', 'VB']\n",
      "[[1 0 1 1 1 0 1 0 1 0 1 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 1 1 0 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#품사 태깅 후 원 핫 인코딩\n",
    "maxims = [\"The harder you work the more likely you can reach the goal\",\n",
    "         \"Believe in yourself\",\n",
    "         \"No pain No gain\",\n",
    "         \"No sweat No sweet\",\n",
    "         \"Courage is very important when it comes to anythin\"]\n",
    "\n",
    "#품사 태깅한 내용을 저장할 list\n",
    "tagged_maxims = []\n",
    "for maxim in maxims:\n",
    "    maxim_tag = nltk.pos_tag(word_tokenize(maxim))\n",
    "    tagged_maxims.append([tag for word, tag in maxim_tag])\n",
    "    \n",
    "#품사 확인\n",
    "for temp in tagged_maxims:\n",
    "    print(temp)\n",
    "\n",
    "#여러 개의 열에 1을 설정할 수 있는 One Hot Encoding\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#각 문장이 어떤 품사들로 구성되어 있는지 확인 가능\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "print(one_hot_multi.fit_transform(tagged_maxims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d57e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d354c1e",
   "metadata": {},
   "source": [
    "### BoW(Bag of Word) - 단어의 등장 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7acb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "[[0 0 0 0 0 1 2 0]\n",
      " [0 1 0 0 1 0 0 1]\n",
      " [1 0 1 1 0 0 0 0]]\n",
      "['beats', 'best', 'both', 'germany', 'is', 'love', 'newziland', 'sweden']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#테스트 데이터 만들기\n",
    "text_data = np.array([\n",
    "    'I love newziland. newziland', \n",
    "    'Sweden is best',\n",
    "    'Germany beats both'\n",
    "])\n",
    "\n",
    "\n",
    "#BoW 특성 행렬 생성\n",
    "count = CountVectorizer()\n",
    "#희소행렬을 생성\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(bag_of_words)\n",
    "\n",
    "#밀집행렬을 출력\n",
    "print(bag_of_words.toarray())\n",
    "\n",
    "#각 열의 이름을 확인\n",
    "print(count.get_feature_names())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6d5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1c06ac8",
   "metadata": {},
   "source": [
    "### tf-idf : 단어의 가중치 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b1f7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.8444932017012523\n",
      "  (0, 6)\t0.5355662725381126\n",
      "  (1, 1)\t0.6176143709756019\n",
      "  (1, 4)\t0.48693426407352264\n",
      "  (1, 9)\t0.6176143709756019\n",
      "  (2, 2)\t0.5773502691896257\n",
      "  (2, 0)\t0.5773502691896257\n",
      "  (2, 3)\t0.5773502691896257\n",
      "  (3, 10)\t0.2747918015856176\n",
      "  (3, 8)\t0.2747918015856176\n",
      "  (3, 11)\t0.8243754047568529\n",
      "  (3, 5)\t0.2747918015856176\n",
      "  (3, 4)\t0.21664901266330147\n",
      "  (3, 7)\t0.21664901266330147\n",
      "{'love': 6, 'newziland': 7, 'sweden': 9, 'is': 4, 'best': 1, 'germany': 3, 'beats': 0, 'both': 2, 'korea': 5, 'top': 11, 'of': 8, 'the': 10}\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.53556627 0.8444932  0.         0.         0.         0.        ]\n",
      " [0.         0.61761437 0.         0.         0.48693426 0.\n",
      "  0.         0.         0.         0.61761437 0.         0.        ]\n",
      " [0.57735027 0.         0.57735027 0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21664901 0.2747918\n",
      "  0.         0.21664901 0.2747918  0.         0.2747918  0.8243754 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#테스트 데이터 만들기\n",
    "text_data = np.array([\n",
    "    'I love newziland. newziland', \n",
    "    'Sweden is best',\n",
    "    'Germany beats both',\n",
    "    'Korea newziland is top of the top top'\n",
    "])\n",
    "\n",
    "#단어의 중요도를 희소행렬로 출력\n",
    "#하나의 문장에서 자주 등장하면 중요도는 높아짐\n",
    "#여러 문장에서 자주 등장하면 중요도는 낮아짐\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "print(feature_matrix)\n",
    "\n",
    "#특성의 이름 파악 \n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "#밀집 행렬로 변환\n",
    "print(feature_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00364a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41855742",
   "metadata": {},
   "source": [
    "# 시계열 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c643a7",
   "metadata": {},
   "source": [
    "## 문자열을 시계열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cada64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    20 non-null     object\n",
      " 1   Close   20 non-null     int64 \n",
      " 2   Start   20 non-null     int64 \n",
      " 3   High    20 non-null     int64 \n",
      " 4   Low     20 non-null     int64 \n",
      " 5   Volume  20 non-null     int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   Date     20 non-null     object        \n",
      " 1   Close    20 non-null     int64         \n",
      " 2   Start    20 non-null     int64         \n",
      " 3   High     20 non-null     int64         \n",
      " 4   Low      20 non-null     int64         \n",
      " 5   Volume   20 non-null     int64         \n",
      " 6   NewDate  20 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5), object(1)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stock = pd.read_csv('./data/stock-data.csv')\n",
    "#첫번째 열인 Date 의 자료형이 object\n",
    "stock.head()\n",
    "stock.info()\n",
    "\n",
    "stock['NewDate'] = pd.to_datetime(stock['Date'])\n",
    "stock.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7c5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
